{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harniaz-Brar/Sentiment-Analysis-and-Text-Mining/blob/main/Group_6_BAN200_Week_01_02_Homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BAN200 Week 01 Homework"
      ],
      "metadata": {
        "id": "tDdPniKEu7ia"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T9-qM_BkuB2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete the homework you will need to modify this template by adding Python code and/or text.\n",
        "\n",
        "Before starting the homework, make sure to save a copy of this template to your personal Google Drive. If you haven't saved your own copy, any changes you make will be lost when you close your browser window.\n",
        "\n",
        "To submit your homework: go to \"File\" in the Colab menu bar > select \"Download\" > select \"Download .ipynb\". This will download a \".ipynb\" file to your computer. You must submit this file.\n",
        "\n",
        "The homework is to be completed in groups. It is due at the start of next class.\n",
        "\n",
        "Homework is graded on the following scale:\n",
        "\n",
        "* *100%* -- The assignment was submitted on time, any code runs without errors, and every question is answered correctly.\n",
        "\n",
        "* *80%* -- The assignment was submitted on time, any code runs without errors, and every question is answered. Some questions may be incorrect, but the submission demonstrates an average level of effort and average level of understanding of the material.\n",
        "\n",
        "* *60%* -- The submission demonstrates a below-average level of effort and below-average level of understanding of the material. This is the highest grade that should be given to submissions that are submitted late, have code that throws uncaught errors, or leave some questions unanswered.\n",
        "\n",
        "* *0%* -- No assignment was submitted, or the submission demonstrates little-to-no effort and little-to-no understanding of the material."
      ],
      "metadata": {
        "id": "Ebn9PTsc6C3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "tYcvY8gEwszK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Represent this tweet as a Python list of word tokens, ignoring any non-alphanumeric characters:\n",
        "```\n",
        "I love McDonalds!\n",
        "```\n",
        "Your list should be stored in a variable `tweet_0`."
      ],
      "metadata": {
        "id": "XXLVXlwxvQAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhuYLJ9ru3UY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c04c8ba3-2337-4393-f2e8-ccbbaef7dccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'love', 'McDonalds']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "tweet = \"I love McDonalds!\"\n",
        "tweet_0 = re.findall('[A-Za-z]+', tweet)\n",
        "print(tweet_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "Gs8s0KVfwvPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Represent this tweet as a Python list of word tokens, ignoring any non-alphanumeric characters:\n",
        "```\n",
        "McDonalds: you are so good ...\n",
        "```\n",
        "Your list should be stored in a variable `tweet_1`."
      ],
      "metadata": {
        "id": "sibMSRSkwvZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "tweet = \"McDonalds: you are so good ...\"\n",
        "tweet_1 = re.findall('[A-Za-z]+', tweet)\n",
        "print(tweet_1)"
      ],
      "metadata": {
        "id": "DPR15yQRxNm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85153b0b-f700-4487-cd30-b13e33774e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['McDonalds', 'you', 'are', 'so', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3"
      ],
      "metadata": {
        "id": "vuhYPiUjyZqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Represent this tweet as a Python list of word tokens, ignoring any non-alphanumeric characters:\n",
        "```\n",
        "This McDonalds hamburger, it is gross\n",
        "```\n",
        "Your list should be stored in a variable `tweet_2`."
      ],
      "metadata": {
        "id": "Nh2axebEyZqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "tweet = \"This McDonalds hamburger, it is gross\"\n",
        "tweet_2 = re.findall('[A-Za-z]+', tweet)\n",
        "print(tweet_2)"
      ],
      "metadata": {
        "id": "RAGTbXTPyZqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8ae123-1d2c-4c29-d30d-b37796b8fb3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'McDonalds', 'hamburger', 'it', 'is', 'gross']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4"
      ],
      "metadata": {
        "id": "rJXKnNWG_Zdr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Represent this lexicon as a Python dictionary called `lexicon`:\n",
        "\n",
        "```\n",
        "+2.1  love\n",
        "+1.8  good\n",
        "-1.5  gross\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nR2dofHV_dge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon = {\n",
        "    \"love\": 2.1,\n",
        "    \"good\": 1.8,\n",
        "    \"gross\": -1.5\n",
        "}"
      ],
      "metadata": {
        "id": "DUFEM1dpA6VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5"
      ],
      "metadata": {
        "id": "ozA3GUqDzXpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a for loop to iterate over all of the tokens in `tweet_2` and make them lower case."
      ],
      "metadata": {
        "id": "CzJ1Ht1-zZw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "for token in tweet_2:\n",
        "    print(token.lower())"
      ],
      "metadata": {
        "id": "4prOUtnDzaOE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aea7f9f8-0c4a-4b05-93aa-16f4b9b3842f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this\n",
            "mcdonalds\n",
            "hamburger\n",
            "it\n",
            "is\n",
            "gross\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6"
      ],
      "metadata": {
        "id": "JwM5Sph5BbYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a for loop to calculate a sentiment score for `tweet_2` using `lexicon`."
      ],
      "metadata": {
        "id": "BtSJ-32yBbYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_score = 0\n",
        "for token in tweet_2:\n",
        "    token_lower = token.lower()\n",
        "    if token_lower in lexicon:\n",
        "        sentiment_score += lexicon[token_lower]\n",
        "print(\"Sentiment score:\", sentiment_score)"
      ],
      "metadata": {
        "id": "ykReYpiwBbYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "911314cf-05cd-4c33-ea92-799f023f3da5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment score: -1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7"
      ],
      "metadata": {
        "id": "DBZ1DuR0B0LN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that takes two parameters -- a list of tokens and a lexicon -- and returns a sentiment score. Test-out your function with `tweet_2`."
      ],
      "metadata": {
        "id": "Eak51Y5gB0LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentiment_score(tokens, lexicon):\n",
        "    sentiment_score = 0\n",
        "    for token in tokens:\n",
        "        token_lower = token.lower()\n",
        "        if token_lower in lexicon:\n",
        "            sentiment_score += lexicon[token_lower]\n",
        "    return sentiment_score\n",
        "\n",
        "score = calculate_sentiment_score(tweet_2, lexicon)\n",
        "print(\"Sentiment score for tweet_2:\", score)"
      ],
      "metadata": {
        "id": "CD_9162rB0LP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b8d35d-5662-440d-edc5-487d3332fa54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment score for tweet_2: -1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8"
      ],
      "metadata": {
        "id": "tzp8AShL02T7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that takes a list of tokens as a paramater and makes them lower case. You can repurpose your code from Question 5. Test-out your function with `tweet_1`."
      ],
      "metadata": {
        "id": "Eu1GJe7Z04RO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_tokens(tokens):\n",
        "    lowercased = []\n",
        "    for token in tokens:\n",
        "        lowercased.append(token.lower())\n",
        "    return lowercased\n",
        "lowercase_tokens_list = lowercase_tokens(tweet_1)\n",
        "print(\"Lowercase tokens for tweet_1:\", lowercase_tokens_list)"
      ],
      "metadata": {
        "id": "I2j6XV3003eK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95867755-94a3-4ceb-f435-db2ee9a1f2d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lowercase tokens for tweet_1: ['mcdonalds', 'you', 'are', 'so', 'good']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9"
      ],
      "metadata": {
        "id": "XbyMF6StxQm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new variable `corpus` that stores the three earlier tweets in a \"list of lists\"."
      ],
      "metadata": {
        "id": "brjrHomcxR3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_0 = [\"I\", \"love\", \"McDonalds\"]\n",
        "tweet_1 = [\"McDonalds\", \"you\", \"are\", \"so\", \"good\"]\n",
        "tweet_2 = [\"This\", \"McDonalds\", \"hamburger\", \"it\", \"is\", \"gross\"]\n",
        "\n",
        "corpus = [tweet_0, tweet_1, tweet_2]\n",
        "\n",
        "print(\"Corpus:\", corpus)"
      ],
      "metadata": {
        "id": "p0D9k0XdzRUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f44190-d214-4ace-bd94-664d47ad6a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus: [['I', 'love', 'McDonalds'], ['McDonalds', 'you', 'are', 'so', 'good'], ['This', 'McDonalds', 'hamburger', 'it', 'is', 'gross']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10"
      ],
      "metadata": {
        "id": "8-6RVuSm-knj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a for loop to iterate over all of the tweets in `corpus` and use your function from Question 8 to make all of the tokens lowercase."
      ],
      "metadata": {
        "id": "obzpKZnn-lv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase_tokens(tokens):\n",
        "    return [token.lower() for token in tokens]\n",
        "tweet_0 = [\"I\", \"love\", \"McDonalds\"]\n",
        "tweet_1 = [\"McDonalds\", \"you\", \"are\", \"so\", \"good\"]\n",
        "tweet_2 = [\"This\", \"McDonalds\", \"hamburger\", \"it\", \"is\", \"gross\"]\n",
        "corpus = [tweet_0, tweet_1, tweet_2]\n",
        "for i in range(len(corpus)):\n",
        "    corpus[i] = lowercase_tokens(corpus[i])\n",
        "print(\"Corpus with lowercase tokens:\", corpus)"
      ],
      "metadata": {
        "id": "nVPLGpPo-00p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a615424c-4a32-4b5a-8cb4-72ed1b92d529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corpus with lowercase tokens: [['i', 'love', 'mcdonalds'], ['mcdonalds', 'you', 'are', 'so', 'good'], ['this', 'mcdonalds', 'hamburger', 'it', 'is', 'gross']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 11"
      ],
      "metadata": {
        "id": "0zZUcayuCh3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a for loop and your function from Question 7 to print-out scores for each tweet."
      ],
      "metadata": {
        "id": "S_1GxD65CjRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentiment_score(tokens, lexicon):\n",
        "    sentiment_score = 0\n",
        "    for token in tokens:\n",
        "        token_lower = token.lower()\n",
        "        if token_lower in lexicon:\n",
        "            sentiment_score += lexicon[token_lower]\n",
        "    return sentiment_score\n",
        "\n",
        "tweet_0 = [\"I\", \"love\", \"McDonalds\"]\n",
        "tweet_1 = [\"McDonalds\", \"you\", \"are\", \"so\", \"good\"]\n",
        "tweet_2 = [\"This\", \"McDonalds\", \"hamburger\", \"it\", \"is\", \"gross\"]\n",
        "\n",
        "corpus = [tweet_0, tweet_1, tweet_2]\n",
        "\n",
        "lexicon = {\n",
        "    \"love\": 2.1,\n",
        "    \"good\": 1.8,\n",
        "    \"gross\": -1.5\n",
        "}\n",
        "\n",
        "def make_lowercase(tokens):\n",
        "    return [token.lower() for token in tokens]\n",
        "\n",
        "for tweet in corpus:\n",
        "    lowercased_tweet = make_lowercase(tweet)\n",
        "    score = calculate_sentiment_score(lowercased_tweet, lexicon)\n",
        "    print(\"Sentiment score for tweet:\", score)"
      ],
      "metadata": {
        "id": "5O9AvoNSCtdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "711c54ce-71c3-43c1-ff7d-274f598b8961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment score for tweet: 2.1\n",
            "Sentiment score for tweet: 1.8\n",
            "Sentiment score for tweet: -1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 12"
      ],
      "metadata": {
        "id": "glJVQiVyDjbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use your function from Question 7 to score the tweet `McDonalds is not great`."
      ],
      "metadata": {
        "id": "H-5hmNnhDlez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentiment_score(tokens, lexicon):\n",
        "    sentiment_score = 0\n",
        "    for token in tokens:\n",
        "        token_lower = token.lower()\n",
        "        if token_lower in lexicon:\n",
        "            sentiment_score += lexicon[token_lower]\n",
        "    return sentiment_score\n",
        "\n",
        "lexicon = {\n",
        "    \"love\": 2.1,\n",
        "    \"good\": 1.8,\n",
        "    \"gross\": -1.5,\n",
        "    \"great\": 1.5,\n",
        "    \"not\": -0.5\n",
        "}\n",
        "\n",
        "tweet = [\"McDonalds\", \"is\", \"not\", \"great\"]\n",
        "\n",
        "score = calculate_sentiment_score(tweet, lexicon)\n",
        "\n",
        "print(\"Sentiment score for the tweet:\", score)\n"
      ],
      "metadata": {
        "id": "ekJKKy3nDkrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a63d4dc-8b2e-483b-de46-bb97aecbc7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment score for the tweet: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 13"
      ],
      "metadata": {
        "id": "4RYUnxt1DuBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In one or two sentences, explain the pros and cons of our sentiment model."
      ],
      "metadata": {
        "id": "wg8XyVSLEA28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pros:\n",
        "Simplicity: The model is easy to implement and understand, requiring only a dictionary of words with associated sentiment scores.\n",
        "Efficiency: It is computationally efficient, making it suitable for quick sentiment analysis on small to moderately sized datasets.\n",
        "Cons:\n",
        "Limited Context Understanding: The model does not handle negations, sarcasm, or context effectively, which can lead to inaccurate sentiment scores.\n",
        "Vocabulary Dependence: It relies heavily on the completeness and accuracy of the lexicon, potentially missing out on important words or phrases not included in the lexicon."
      ],
      "metadata": {
        "id": "_bu-g5h7EO8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "n871PZxNuTor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 14\n"
      ],
      "metadata": {
        "id": "JuaYuGhuuF14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that takes a string and returns a list of word tokens, ignoring any non-alphanumeric characters. You should use re.findall in your function. Test your function with the string \"Hello there, how are you? Want to go skiing?\" and store the tokens in a variable tokens."
      ],
      "metadata": {
        "id": "2tu20-caufJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize_string(input_string):\n",
        "    # Use re.findall to find all alphanumeric word tokens\n",
        "    tokens = re.findall('[A-Za-z]+', input_string)\n",
        "    return tokens\n",
        "\n",
        "# Test the function with the provided string\n",
        "test_string = \"Hello there, how are you? Want to go skiing?\"\n",
        "tokens = tokenize_string(test_string)\n",
        "\n",
        "# Print the tokens\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "id": "DM7oaarIukLA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7232240a-4538-47a1-c635-c548d5466507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Hello', 'there', 'how', 'are', 'you', 'Want', 'to', 'go', 'skiing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 15\n"
      ],
      "metadata": {
        "id": "rHwCQr0cu_lB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function that takes a list of tokens as a parameter and replaces them with their lemma if that token exists in the lemmas variable. Test it with your tokens variable."
      ],
      "metadata": {
        "id": "_xQ3ADcRvPpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = {\n",
        "    \"hello\": \"greeting\",\n",
        "    \"there\": \"there\",\n",
        "    \"how\": \"how\",\n",
        "    \"are\": \"be\",\n",
        "    \"you\": \"you\",\n",
        "    \"Want\": \"want\",\n",
        "    \"to\": \"to\",\n",
        "    \"go\": \"go\",\n",
        "    \"skiing\": \"ski\"\n",
        "}\n",
        "\n",
        "def replace_with_lemma(tokens, lemmas):\n",
        "    for i in range(len(tokens)):\n",
        "        if tokens[i] in lemmas:\n",
        "            tokens[i] = lemmas[tokens[i]]\n",
        "    return tokens\n",
        "\n",
        "tokens = ['Hello', 'there', 'how', 'are', 'you', 'Want', 'to', 'go', 'skiing']\n",
        "tokens_with_lemmas = replace_with_lemma(tokens, lemmas)\n",
        "\n",
        "print(\"Tokens with lemmas:\", tokens_with_lemmas)\n"
      ],
      "metadata": {
        "id": "MJkPOymA8VpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d349bd05-7f74-461a-be96-fd2a48087b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens with lemmas: ['Hello', 'there', 'how', 'be', 'you', 'want', 'to', 'go', 'ski']\n"
          ]
        }
      ]
    }
  ]
}